import re
!pip install langdetect
from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM
from datetime import datetime
from langdetect import detect
from difflib import SequenceMatcher

# Carregar o classificador de jailbreak
classifier = pipeline("text-classification", model="jackhhao/jailbreak-classifier")

# Carregar o modelo generativo TinyLlama
tokenizer_llama = AutoTokenizer.from_pretrained("TinyLlama/TinyLlama-1.1B-Chat-v1.0")
model_llama = AutoModelForCausalLM.from_pretrained("TinyLlama/TinyLlama-1.1B-Chat-v1.0")

# Pipeline de geração com TinyLlama
generator = pipeline("text-generation", model=model_llama, tokenizer=tokenizer_llama)

# Função de log para auditoria de inputs maliciosos
def log_malicious_prompt(prompt, level):
    with open("malicious_prompts_log.txt", "a") as log_file:
        log_file.write(f"{datetime.now()} - Nível de risco: {level} - Prompt malicioso detectado: {prompt}\n")

# Função para verificar a similaridade entre palavras
def is_similar(word, keyword, threshold=0.8):
    return SequenceMatcher(None, word, keyword).ratio() > threshold

# Função para verificar se o prompt é uma tentativa de jailbreak e verificar palavras-chave com variações
def check_jailbreak_and_keywords(prompt):
    # Verificar se o classificador detecta o prompt como malicioso
    result = classifier(prompt)
    label = result[0]['label']
    
    # Se o classificador identificar como malicioso, bloquear
    if label == 'malicious':
        log_malicious_prompt(prompt, 'Alto Risco')
        return True, 'Alto Risco'  # Tentativa de jailbreak grave detectada
    
    # Verificação manual com palavras-chave e variações
    dangerous_keywords = [
        "phishing", "hack", "fraud", "bank", "ddos", "crash", "exploit", "malware", 
        "ransomware", "keylogger", "overflow", "brute force", "mitm", "sniffing", 
        "spoofing", "injection", "xss", "remote code execution", "cross-site scripting", 
        "session hijacking", "sql injection"
    ]
    
    words_in_prompt = prompt.lower().split()
    
    # Verificação de palavras exatas e variações
    for word in words_in_prompt:
        for keyword in dangerous_keywords:
            if is_similar(word, keyword) or re.search(fr"\b{keyword}\b", word):
                log_malicious_prompt(prompt, 'Baixo Risco')
                return True, 'Baixo Risco'  # Detectou tentativa de ataque por palavras-chave
    
    return False, 'Seguro'  # Prompt seguro

# Função para processar o prompt e repassar ao modelo generativo, com feedback ao usuário
def process_prompt(prompt):
    # Verifica se o prompt é malicioso usando o classificador e a verificação de palavras-chave
    is_malicious, risk_level = check_jailbreak_and_keywords(prompt)
    
    if is_malicious:
        # Mensagens detalhadas de acordo com o nível de malícia detectado
        if risk_level == 'Alto Risco':
            return (
                "O prompt enviado contém palavras ou intenções altamente perigosas, "
                "sendo classificado como uma tentativa grave de jailbreak. Por questões de segurança, "
                "não podemos processar essa solicitação."
            )
        else:
            return (
                "O prompt enviado contém possíveis palavras ou padrões arriscados, sendo classificado como de "
                "baixo risco. Recomendamos que reformule sua solicitação com termos mais neutros para que possa ser processada."
            )
    
    # Se o prompt for seguro, gera uma resposta usando o modelo TinyLlama
    response = generator(prompt, max_new_tokens=50)
    return response[0]['generated_text']

# Função para suporte multilíngue, detectando o idioma do prompt
def detect_language(prompt):
    # Detectar idioma com langdetect
    lang = detect(prompt)
    
    if lang == 'en':
        return 'Inglês'
    elif lang == 'pt':
        return 'Português'
    else:
        return 'Outro Idioma'

# Função para fornecer feedback ao usuário baseado no idioma detectado
def provide_feedback(prompt):
    language = detect_language(prompt)
    
    if language == 'Português':
        return "Seu prompt foi identificado em Português. Processando solicitação..."
    elif language == 'Inglês':
        return "Your prompt is in English. Processing your request..."
    else:
        return "Idioma não identificado. Processando solicitação..."

# Função principal que integra todas as funcionalidades
def main():
    print("Bem-vindo ao sistema de processamento de prompts!")
    while True:
        user_prompt = input("Digite seu prompt (ou 'sair' para encerrar): ")
        
        if user_prompt.lower() == 'sair':
            print("Encerrando interação. Até logo!")
            break
        
        # Fornece feedback baseado no idioma do prompt
        print(provide_feedback(user_prompt))
        
        # Processa o prompt e gera a resposta apropriada usando TinyLlama
        response = process_prompt(user_prompt)
        print("Resposta:", response)

# Executa o sistema
main()
